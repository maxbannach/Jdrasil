\documentclass[a4paper, ukenglish, twoside, openright]{jdrasilmanual}

\setversion{0.1}
\author{Max Bannach, Sebastian Berndt, Thorsten Ehlers}

\begin{document}
\part{Upgrades}

\Jdrasil\ is designed as a modular and platform independent library,
which provides all the advantages discussed earlier, but also comes
with a couple of problems. In particular, in order to compute a tree
decomposition of a graph, \Jdrasil\ internally solves many different
combinatorial optimization problems. Some of these problem may be
solved more efficiently on a specific target platform, rather then on
Javas virtual machine. For instance, one may want to use present
graphic cards for massive parallelization. On the other hand, for many
of these problems there are excellent and optimized libraries
available, which we want to use. For instance, \Jdrasil\ will rather
use existing \Lang{SAT} solvers to solve the boolean satisfiability
problem, instead of implementing its own. The concept of
\emph{upgrades} is \Jdrasil's way to use external code or
libraries. 

We use the term ``upgrade'', in contrast to something like
``library'', as \Jdrasil\ will always be fully functional and platform
independent without any upgrade. In particular, it can be compiled and
shipped without any upgrade. On the other hand, an upgrade will, as
the name suggests, speed up \Jdrasil\ on certain instances or
platforms. In other words, an upgrade will not increase the
functionality of \Jdrasil, but will provide tools \emph{for} \Jdrasil\
such that it can execute its functionality faster.

The default location of upgrades for \Jdrasil\ is the folder
\file{upgrades} next to \file{jdrasil.jar}. Since \Jdrasil\ comes
without any upgrade, this folder, at default, does not contain
much. However, the \file{Makefile} of \Jdrasil\ provides some targets
to obtain upgrades.

\chapter{Boolean Satisfiability}
The boolean satisfiablity problem \Lang{SAT} is the most canonical
$\Class{NP}$-complete problem, and ``simply'' asks if a boolean
formula in CNF has a satisfying model. Many $\Class{NP}$-complete
problems can naturally be stated as a \Lang{SAT}-problem, and hence, can
be naturally solved by finding a model for a CNF formula. This is the
reason why \emph{SAT solvers}, i.\,e., tools that solve the boolean
satisfiablity problem, have received a lot of research afford. In
particular, there are annual challenges\footnote{\url{http://baldur.iti.kit.edu/sat-competition-2016/}} that try to
find the fastest solver. The result of this afford is that modern
SAT solver can solve hard problems on many instances very quickly.

The power of SAT solver makes it interesting to use them while
computing a tree decomposition. Equipped with a SAT solver, \Jdrasil\
can directly encode the problem of finding a tree decomposition (or
more precisely, an elimination order) into a $\Lang{SAT}$-formula. See
section~\ref{???} for more details. On the other hand, \Jdrasil\ can
also use the SAT solver to solve different subproblems while computing
the tree decomposition. For instance, if \Jdrasil\ computes an
elimination order, it can always put a clique of the graph at the
end of the permutation. If the clique is big, this can reduce the
search space dramatically. However, finding big cliques in graph is
$\Class{NP}$-hard as well. \Jdrasil\ can use a SAT solver to find the
largest clique in the graph.

\section{The Formula Class}\label{section:satFormula}
The main interface of \Jdrasil\ to use boolean logic is the class
\JClass{jdrasil.sat.Formula}, which represents a boolean formula in
CNF. This class is always available and can always be used to create
and mange logic formulas. 

\subsection{Specifying a Formula}
A formula $\phi$ is always represented in CNF and in the classic
DIMACS format, that is, variables are positive integers
$x\in\mathbb{N}$, and negated variables are simply stored $-x$. We
can specify a formula by adding clauses to it, for instances
$\phi=(x_1\vee\neg x_2\vee x_3)\wedge(x_2\vee\neg x_3)\wedge x_3$ can
be created as follows:

\begin{lstlisting}[language=Java]
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
\end{lstlisting}

We can also ``concatenate'' two formulas by combine them with a logic
``and'', i.\,e., we can compute $\phi\wedge\psi$:

\begin{lstlisting}[language=Java]
  Formula psi = new Formula();
  psi.addClause(-1);
  
  phi.and(psi);
\end{lstlisting}

We can always add clauses to an existing formula or concatenate it
with another formula. With other words, we can always further
restrict the solution space of a formula. Sometimes, however, we may
wish to remove an clause, which can be done by:
\begin{lstlisting}[language=Java]
  psi.removeClause(-1);
\end{lstlisting}
But this operation should be used with caution: first of all it is
much more expensive to remove a clause then adding one; and,
furthermore, we are not always allowed to remove a clause (the method
can throw an exception). The reason for this is that SAT solvers that
solve a formula incrementally often only allow to restrict the formula
further. This means, once we started to ``solve'' the formula, we
can not remove clauses anymore.

\subsection{Solving a Formula}
So, \note{We can only solve formulas if a SAT solver is installed as upgrade.}
how to actually find a model for the formula, i.\,e., how to
``sovle'' it. In order to check if a formula has a satisfying
assignment, \Jdrasil\ uses external SAT solvers which have to be
installed as upgrade (see the following sections for possible
solvers). If a SAT solver is installed, we can register it to the
formula:
\begin{lstlisting}[language=Java]
  String sig = phi.registerSATSolver();
\end{lstlisting}
This method will register an arbitrary SAT solver that \Jdrasil\ has
found as upgrade. If no SAT solver is installed, this method will
throw an exception; otherwise the signature of the solver is
returned. Once a solver is registered, the following will happen:
\begin{enumerate}
  \item The formula ``as is'' will be transfered to the solver,
    i.\,e., all clauses stored will be send to the solver.
  \item The formula and the solver will be kept in sync, that is,
    clauses added to the formula will directly be added to the solver.
  \item The method \JMethod{removeClause()} can not be called
    anymore.
  \item The method \JMethod{isSatisfiable()} can now be called.
\end{enumerate}

Once a solver was registered to the formula, we can check if there is
a satisfying assignment:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable();
\end{lstlisting}
This method will use the SAT solver to solve the formula. The whole
API is incremental, so we can modify the formula between calls of
this method (which will be faster then recreating new formulas). A
typical scenario would look like:
\begin{lstlisting}[language=Java]
  while (phi.isSatisfiable()) {
    phi.addClause(...);
    ...
  }
\end{lstlisting}
Sometimes, we actually would like to remove clauses between calls
(which we are not allowed to do, as mentioned earlier). To overcome
this issue, most incremental SAT solvers support the concept of
\emph{assumptions}. An assumption is an unit clause that is added to
the solver for a single run. We can for instances say
$x_1=\mathrm{true}$ and check if the formula is satisfiable
\emph{under this assumption}. After a
call of \JMethod{isSatisfiable()}, all assumptions are removed.
To check if a formula is satisfiable under a set of assumptions,
simply add them to the method call:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable(1, -3);
\end{lstlisting}
Once we have defined a formula and solved it using
\JMethod{isSatisfiable()}, we are most likely interested in an actual
satisfying model. A model is a mapping from the variables to boolean
values, i.\,e., a \JClass{Map<Integer, Boolean>} and can be obtained
with the following call:
\begin{lstlisting}[language=Java]
  Map<Integer, Boolean> model = phi.getModel();
  System.out.printf("Value of %d is %b\n", 1, model.get(1));
  System.out.printf("Value of %d is %b\n", 2, model.get(2));
  System.out.printf("Value of %d is %b\n", 3, model.get(3));
\end{lstlisting}
Note that we can only obtain a model after a call to
\JMethod{isSatisfiable()}, and only if this call has returned
true. Otherwise the code from above will throw an exception.

\subsection{Auxiliary Variables}
When we model a problem as CNF formula, we often need a lot of
additional variables, which do not directly model parts of the problem
(as vertex is selected or not), but that model structural things of
the formula (to allow us to write them in short CNF). These variables
arise a lot and will be added by different methods to the
formula. However, if we talk about the formula on a higher level, we
actually do not want these variables. For instance, we do not want
have variables in our model that we do not know.

\Jdrasil\ provides the concept of \emph{auxiliary variables} to mark
variables as helper variables, that are not directly connected to the
modeled problem. The variable $x_3$ can be marked as auxiliary with the
following command:
\begin{lstlisting}[language=Java]
  phi.markAuxiliary(3);
\end{lstlisting}
Once a variable $x$ is marked as being auxiliary, the following will
happen:
\begin{enumerate}
  \item The variable list of the formula will not contain $x$.
  \item A model will not contain an entry for $x$.
  \item The auxiliary variable list will contain $x$.
  \item The behavior of the formula, a registered SAT solver, and the
    satisfiability of the formula will \emph{not} change. The variable
    is still part of the formula.
\end{enumerate}

\subsection{Cardinality Constraint}
Many problems can naturally be encoded into an CNF~–~\emph{when} we can
restrict the number of variables that we are allowed to set to
true. For instance, a \emph{vertex cover} of a graph is a subset of
its vertices, such that every edge is incident to one of these
vertices. The graph at the border, for instance, has the vertex cover $\{2,4\}$.
\marginpar{
  \tikz[]\graph[spring electrical layout, node distance=0.75cm]{1--2--{3,4},4--{5,6}};
}
For a given graph $G=(V,E)$, it is easy to write down a formula that
states that the graph has a vertex cover:
\[
   \phi=\bigwedge_{\{u,v\}\in E}(x_u\vee x_v).
\]
However, as simple this formula is, as uninteresting it is as well:
every graph contains a vertex cover~–~just take all the vertices. To make
the problem interesting (and difficult), we have to restrict the
number of vertices that we are allowed to set to true. This is exactly
what a \emph{cardinality constraint} does.

\Jdrasil\ provides two ways to add cardinality constraints to an
formula. In both cases, we first of all need so specify the set of
variables (or literals) that we wish to restrict:
\begin{lstlisting}[language=Java]
  // build the formula
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
  
  // define a set that we wish to restrict
  Set<Integer> vars = new HashSet<>();
  vars.add(1);
  vars.add(2);
  vars.add(3);
\end{lstlisting}
Note that $\phi$ is satisfiable and has only one model, which sets all
variables to true. So we get:

\codeWithOutput{phi.isSatisfiable()}{true}

We can now restrict the number of variables that are allowed to be set
to true, for instance, to $2$:
\begin{lstlisting}[language=Java]
  phi.addAtMost(2, vars);
\end{lstlisting}
We now obtain, as expected:

\codeWithOutput{phi.isSatisfiable()}{false}

In a similar manner, we can also enforce that a certain amount of
variables \emph{must be set}, but for our formula this has no effect:
\begin{lstlisting}[language=Java]
  phi.addAtLeast(2, vars);
\end{lstlisting}
Both methods, \detail{These methods use sequential counters and
  introduced \textcolor{jdrasil.fg}{$O(kn)$} auxiliary variables \emph{per call}.}
\JMethod{addAtMost} and \JMethod{addAtLeast}, add
clauses and auxiliary variables to the formula. This should be used
for cardinality constraints that are used only once, since these
methods will add these clauses for every call again (even if the set
of variables does not change).

However, when we solve an optimization problem, we often wish to add a
cardinality constraint for the same set of variables again and
again. For instances, a typical routine to solve vertex cover would
look like:
\begin{lstlisting}[language=Java]
  Formula phi = ...       // as in the example
  phi.registerSolver();

  Set<Integer> vars = ... // all variables
  int k = vars.size() - 1;

  phi.addAtMost(k, vars);
  while (phi.isSatisfiable()) {
    k = k - 1;
    phi.addAtMost(k, vars);
  }

  System.out.println(k+1);
\end{lstlisting}
In such an incremental setup, the methods from above are not optimal,
since they would add the similar auxiliary clauses and variables over
and over again. To overcome this, \Jdrasil\ also provides
\emph{incremental cardinality constraints}. The following ensures that
at least $3$ and at most $6$ variables of the set \JMethod{vars} is
set to true:
\begin{lstlisting}[language=Java]
  phi.addCardinalityConstraint(3,6,vars);
\end{lstlisting}
This \detail{This method uses sorting networks and introduced
  \textcolor{jdrasil.fg}{$O(n\log^2 n)$} auxiliary variables overall.}
method will add a lot of auxiliary variables and clauses (most of
the time more then a single call for of \JMethod{addAtMost}), however,
it will reuse this. More precisely, while the first call adds a lot of
structure to the formula, incremental calls will only add single
clauses. So for the algorithm from above, this method is way more
efficient. 

Finally, \Jdrasil\ provides a third possibility to use cardinality
constraints. While computing tree decomposition's, we often deal with
instances of small tree width (as the usual use case is
parameterized complexity). If $k$ is much smaller then $n$, a sorting
network is asymptotically not optimal in sense of introduced auxiliary
variables. For such scenarios, the \JClass{Formula} class provides the
method \JMethod{addDecreasingAtMost}.\detail{This method uses a
  variant sequential counter as well. It introduces \textcolor{jdrasil.fg}{$O(kn)$} auxiliary
  variables overall.}This method can be seen as a compromise between
\JMethod{addAtMost} (which is simple, but static), and
\JMethod{addCardinalityConstraint} (which is complex, but can be
decreased and increased between solver calls). In contrast, the method
\JMethod{addDecreasingAtMost} is as simple as the first method, but
allows to be decreased between calls to the solver. It is, however,
only efficient for small values of $k$; and only works for decreasing
upper bounds (and not increasing lower bounds). Note that we can
interstate this as a parameterized SAT encoding, where $k$ is the
parameter.

\section{SAT4J}
The Java Library SAT4J\footnote{\url{http://www.sat4j.org}} is the
most advanced and complete \Lang{SAT}-library for the Java
platform. Although it is not the fastest solver available, it is one
of the most widespread solver, as it has a clean API and a good
documentation.

\Jdrasil\ implements core functionality of SAT4J completely over
reflections. This is done in the \emph{intern} class
\JClass{jdrasil.sat.SAT4JSolver}, which is an an
\JClass{jdrasil.sat.ISATSolver}. If the SAT4J library is found in
\Jdrasil's classpath, this class will be used by
\JClass{jdrasil.sat.Formula} (see~\ref{section:satFormula}) to find a
model. This is fully capsuled from the user, which only has to work
with the formula class. 

If SAT4J is available in the classpath,
\JMethod(canRegisterSATSolver()) of \JClass{jdrasil.sat.Formula} will
return true, if SAT4J is also used (this depend on \Jdrasil\ and other
loaded upgrades), the method \JMethod{init()} will return the String
``SAT4J''. 

\subsection{Installation}
To use SAT4J in \Jdrasil, it is sufficient to download the core
library\footnote{\url{http://forge.objectweb.org/project/showfiles.php?group_id=228}}.
We can perform this step automatically with \lstinline{make upgrade-sat4j}.

\subsection{Usage}
Just add the SAT4J core library to the classpath:
\begin{lstlisting}
  java -cp bin:upgrades/org.sat4j.core.jar jdrasil.App
\end{lstlisting}
Of course, the SAT4J library can be stored at another location as
well. Other then that, just work with the class \JClass{jdrasil.sat.Formula}
as we would otherwise.

\section{Native IPASIR Solver}

Today's most advanced SAT solvers are mostly implemented in
C/C++. \Jdrasil\ can use such ``native'' solver with the help of Javas
JNI-API\footnote{\url{https://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/jniTOC.html}}. 
To be as general as possible and, thus, to support as many SAT solvers
as possible, \Jdrasil\ implements the
IPASIR\footnote{\url{http://baldur.iti.kit.edu/sat-race-2015/downloads/ipasir.h}}
interface, which is the reversed acronym for ``Re-entrant Incremental
Satisfiability Application Program Interface''. This interface was
proposed and used in recent incremental sat challenges. 

A solver that implements the IPASIR interface just has to implement
the following 9 functions:
\begin{lstlisting}[language=c]
  const char* ipasir_signature();
  void* ipasir_init();
  void ipasir_release(void* solver);
  void ipasir_add(void* solver, int lit_or_zero);
  void ipasir_assume(void* solver, int lit);
  int ipasir_solve(void* solver);
  int ipasir_val(void* solver, int lit);
  int ipasir_failed(void* solver, int lit);
  void ipasir_set_terminate(void* solver, 
                            void* state, 
                            int (*terminate)(void* state));
\end{lstlisting}
More details about what these functions should do can be found on the
website of recent sat challenges. The interface is closely related to
the API of modern solvers as Lingeling or PicoSAT, so that such
solvers can easily be linked against IPASIR.

\Jdrasil\ can use an IPASIR solver as upgrade using the class
\JClass{jdrasil.sat.NativeSATSolver}, which implements the interface
\JClass{jdrasil.sat.ISATSolver} with native methods. Note how this
interface, which we also use for other solvers like SAT4J, is closely
related to IPASIR. 

The corresponding C interface of \JClass{jdrasil.sat.NativeSATSolver} can be found
in the header file \file{jdrasil_sat_NativeSATSolver.h}, which is
stored in \file{upgrades/ipasir}.\note{We can always create this file with
\lstinline[basicstyle=\footnotesize\codefamily]{make cinterface}} The
corresponding C++ implementation
\file{jdrasil_sat_NativeSATSolver.cpp} implements these methods and
maps them against \file{ipasir.h}. This implementation takes care of
keeping \Jdrasil\ and the actual solver in sync, allowing \Jdrasil\ to
use multiple ``instances'' of the solver, and allows \Jdrasil\ to kill
the solver. 

\subsection{Installation} 
To compile an IPASIR upgrade for \Jdrasil, we have to compile the JNI
implementation in the file \file{jdrasil_sat_NativeSATSolver.cpp}
into an dynamic library, wish either should be be called
\file{libjdrasil_sat_NativeSATSolver.so} or, depending on you
operating system,
\file{libjdrasil_sat_NativeSATSolver.dylib}. In order to do so, we
have to link against an exiting implementation of an IPASIR solver.

In \file{upgrades/ipasir} there is an example \file{Makefile} that compiles
C++-file under the assumption that there
is an library \file{libipasirsolver.dylib}.

\todo{There should be make targets to download and compile (platform generic) some
  predefined native solvers (from an alternative git-repo. For
  instance lingeling, picosat, glucose}

\subsection{Usage}
Once we have compiled \Jdrasil's IPASIR-JNI interface against an
IPASIR solver, i.\,e., once we have an library called
\file{libjdrasil_sat_NativeSATSolver.dylib}, we can \emph{upgrade}
\Jdrasil\ ``on the fly''. \Jdrasil\ will look for the SAT solver in
its library path (not to be confused with its class path), so the
following call will allows \Jdrasil\ to use the solver:
\begin{lstlisting}[language=bash]
  java -cp bin -Djava.library.path=upgrades/ipasir jdrasil.App
\end{lstlisting}
Of course, the path can be set to any location, wherever the upgrade
is stored. Note that this only sets the path to location at which
\Jdrasil\ searches the upgrade. If, however, the upgrade is compiled
against other dynamic libraries, these libraries are searched in the
default system depend way (and not in the above specified path).

\end{document}